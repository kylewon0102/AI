{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f5144b-f74c-49a2-b1c0-d8d3590e7fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 00:05:15.401810: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967c83c-03a0-4f5f-9650-bf6c2b3fd5fc",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b56b8bf2-d30f-4942-bab5-5c5d265c9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[2, 0], [4, 0], [6, 0], [8, 1], [10, 1], [12, 1], [14, 1]]\n",
    "x_data = [x_row[0] for x_row in data]\n",
    "y_data = [y_row[1] for y_row in data]\n",
    "\n",
    "a = tf.Variable(tf.random_normal([1], dtype=tf.float64, seed=0))\n",
    "b = tf.Variable(tf.random_normal([1], dtype=tf.float64, seed=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1a0806d-d3f0-41dd-91ed-ff7940159283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x5/47g9ybwn3dj2qvmd4l8th0x80000gn/T/ipykernel_8322/2318995901.py:15: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"Epoch: %.f, loss = %.4f, slope a = %.4f, y-intercept = %.4f\" % (i, sess.run(loss), sess.run(a), sess.run(b)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss = 4.0817, slope a = 2.4706, y-intercept = -0.3620\n",
      "Epoch: 6000, loss = 0.0152, slope a = 2.9230, y-intercept = -20.3114\n",
      "Epoch: 12000, loss = 0.0081, slope a = 3.5648, y-intercept = -24.8081\n",
      "Epoch: 18000, loss = 0.0055, slope a = 3.9564, y-intercept = -27.5511\n",
      "Epoch: 24000, loss = 0.0041, slope a = 4.2385, y-intercept = -29.5268\n",
      "Epoch: 30000, loss = 0.0033, slope a = 4.4590, y-intercept = -31.0705\n",
      "Epoch: 36000, loss = 0.0028, slope a = 4.6399, y-intercept = -32.3371\n",
      "Epoch: 42000, loss = 0.0024, slope a = 4.7933, y-intercept = -33.4107\n",
      "Epoch: 48000, loss = 0.0021, slope a = 4.9263, y-intercept = -34.3424\n",
      "Epoch: 54000, loss = 0.0019, slope a = 5.0439, y-intercept = -35.1653\n",
      "Epoch: 60000, loss = 0.0017, slope a = 5.1491, y-intercept = -35.9020\n",
      "[3.88235158e-05]\n",
      "[0.53537986]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "#sigmoid\n",
    "y = 1/(1+np.e**-(a*x_data + b))\n",
    "\n",
    "#loss\n",
    "loss = -tf.reduce_mean(np.array(y_data) * tf.log(y) + (1-np.array(y_data))* tf.log(1-y))\n",
    "\n",
    "learning_rate = 0.5\n",
    "gradient_descent = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(60001):\n",
    "        sess.run(gradient_descent)\n",
    "        if i  % 6000 == 0:\n",
    "            print(\"Epoch: %.f, loss = %.4f, slope a = %.4f, y-intercept = %.4f\" % (i, sess.run(loss), sess.run(a), sess.run(b)))\n",
    "\n",
    "    calc_a = sess.run(a)\n",
    "    calc_b = sess.run(b)\n",
    "\n",
    "def NewDatacalc(new_x_data):\n",
    "    return 1 / (1 + np.e**-(calc_a * new_x_data + calc_b))\n",
    "\n",
    "print(NewDatacalc(5))\n",
    "print(NewDatacalc(7))\n",
    "print(NewDatacalc(13))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c55a6a-7631-4ee6-86e6-919ffb9b65e4",
   "metadata": {},
   "source": [
    "Multiple Logistic Regression\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59e2c575-4914-417a-80e9-821831bd591d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=300, a1=[1.04453619], a2=[-0.8111726], b=[-2.78782518], loss=0.24920464762582648\n",
      "step=600, a1=[0.93470692], a2=[-0.41186888], b=[-4.10121265], loss=0.18541079345399925\n",
      "step=900, a1=[0.80168377], a2=[-0.03562521], b=[-5.09986046], loss=0.14665520570049115\n",
      "step=1200, a1=[0.67540874], a2=[0.2960461], b=[-5.90621985], loss=0.1206863657982371\n",
      "step=1500, a1=[0.56286146], a2=[0.58425401], b=[-6.58233881], loss=0.10220458438379738\n",
      "step=1800, a1=[0.46453787], a2=[0.8348311], b=[-7.1643961], loss=0.08846481070224202\n",
      "step=2100, a1=[0.379027], a2=[1.05397569], b=[-7.67537416], loss=0.07789698477150467\n",
      "step=2400, a1=[0.30449816], a2=[1.24713424], b=[-8.13076902], loss=0.06954206209936582\n",
      "step=2700, a1=[0.2392001], a2=[1.41881062], b=[-8.54151066], loss=0.0627847181576291\n",
      "step=3000, a1=[0.1816104], a2=[1.57264621], b=[-8.91559007], loss=0.05721427079587034\n",
      "predicted= [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "check predicted= [[0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "check hypothesis= [[0.29531881]\n",
      " [0.68204343]\n",
      " [0.41936017]]\n",
      "\n",
      " Hypothesis:  [[0.02115441]\n",
      " [0.03012927]\n",
      " [0.17716511]\n",
      " [0.87798135]\n",
      " [0.98034251]\n",
      " [0.99711527]\n",
      " [0.99958276]] \n",
      "Correct(Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "공부한 시간: [7.] , [6.]\n",
      "합격 가능성:  85.72%\n",
      "예측 결과: [1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x5/47g9ybwn3dj2qvmd4l8th0x80000gn/T/ipykernel_8322/1150720325.py:57: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"합격 가능성: %6.2f%%\" %(new_y*100))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "\n",
    "#시드는 실행할 때마다 같은 결과를 출력해줌\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# x, y의 데이터 값\n",
    "x_data = np.array([[2, 3], [4, 3],[6, 4],[8, 6], [10, 7], [12, 8], [14, 9]])\n",
    "y_data = np.array([0, 0, 0, 1, 1, 1, 1]).reshape(7, 1) #[0], [0], [0], [1], [1], [1], [1]\n",
    "\n",
    "#입력 값을 플레이스 홀더에 저장\n",
    "X = tf.placeholder(tf.float64, shape=[None, 2]) #x 두개 짜리 데이터 그릇\n",
    "Y = tf.placeholder(tf.float64, shape=[None, 1]) #y 한개 짜리 데이터 그릇\n",
    "\n",
    "#a 안에 a1 a2 값 모두 저장\n",
    "a = tf.Variable(tf.random_normal([2, 1], dtype=tf.float64))\n",
    "b = tf.Variable(tf.random_normal([1], dtype=tf.float64))\n",
    "\n",
    "#y 시그모이드 함수 방정식 사용\n",
    "y = tf.sigmoid(tf.matmul(X, a)+ b)\n",
    "\n",
    "#오차 함수\n",
    "loss = -tf.reduce_mean(Y*tf.log(y) + (1 - Y) * tf.log(1-y))\n",
    "\n",
    "#학습률 값\n",
    "learning_rate = 0.1\n",
    "\n",
    "#오차 최소화 값\n",
    "gradient_descent = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "predicted = tf.cast(y>0.5, dtype=tf.float64)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float64))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(3001):        \n",
    "        a_, b_, loss_, _ = sess.run([a, b, loss, gradient_descent], feed_dict = {X: x_data, Y: y_data})\n",
    "        if (i+1) % 300 == 0:\n",
    "            print(f'step={i+1}, a1={a_[0]}, a2={a_[1]}, b={b_}, loss={loss_}')\n",
    "    \n",
    "    print(\"predicted=\", sess.run(predicted, feed_dict={X:x_data}))\n",
    "    p_val, h_val = sess.run([predicted, y], feed_dict={X:[[1, 5], [10, 5], [4, 5]]})\n",
    "    print(\"check predicted=\", p_val)\n",
    "    print(\"check hypothesis=\", h_val)\n",
    "\n",
    "    h, c, a = sess.run([y, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\n Hypothesis: \", h, \"\\nCorrect(Y): \", c, \"\\nAccuracy: \", a)\n",
    "\n",
    "    new_x = np.array([7, 6.]).reshape(1, 2)\n",
    "    new_y, new_y_result = sess.run([y, predicted], feed_dict={X: new_x})\n",
    "\n",
    "    print(f\"공부한 시간: {new_x[:, 0]} , {new_x[:, 1]}\")\n",
    "    print(\"합격 가능성: %6.2f%%\" %(new_y*100))\n",
    "    print(f\"예측 결과:\", new_y_result[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22f8a443-9d9b-486c-8ec7-4565e3bcfe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53e79ee6-8822-4f80-922e-321e6a296580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True False  True  True  True  True]\n",
      "[1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "\n",
    "Y = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1 ]\n",
    "y_data = [0.1, 0.2, 0.3, 0.1, 0.2, 0.6, 0.7, 0.8, 0.6, 0.9]\n",
    "\n",
    "y = np.array(y_data)\n",
    "predicted = tf.cast(y>0.5, dtype=tf.float64)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float64))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    temp1 = sess.run(tf.equal(predicted, Y))\n",
    "    print(temp1)\n",
    "    temp2 = sess.run(tf.cast(tf.equal(predicted, Y), dtype=tf.float64))\n",
    "    print(temp2)\n",
    "    result = sess.run(accuracy)\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
