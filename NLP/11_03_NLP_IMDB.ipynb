{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24c48833-cbf4-411b-85b6-5494c8f3f2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datas/imdb/train/neg\n",
      "./datas/imdb/train/pos\n",
      "Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\n",
      "0\n",
      "For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "imdb_dir = './datas/imdb/'\n",
    "\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    print(dir_name)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            with open(os.path.join(dir_name, fname), encoding = 'utf8') as f:\n",
    "                texts.append(f.read())\n",
    "            labels.append(0 if label_type == 'neg' else 1)\n",
    "\n",
    "print(texts[0])\n",
    "print(labels[0])\n",
    "print(texts[12500])\n",
    "print(labels[12500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "debf704d-be97-46ed-b889-e742a5f51772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n",
      "Size of Data Tensor: (25000, 200)\n",
      "size of Label Tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 200 #over 200 words, thrown out\n",
    "training_samples = 10000\n",
    "validations_samples = 10000\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "print('Size of Data Tensor:', data.shape)\n",
    "print('size of Label Tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d830ae2-942b-4d28-a083-cd0cf378ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 345ms/step - acc: 0.5874 - loss: 0.6599 - val_acc: 0.7573 - val_loss: 0.5259\n",
      "Epoch 2/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 343ms/step - acc: 0.8244 - loss: 0.4282 - val_acc: 0.7279 - val_loss: 0.5300\n",
      "Epoch 3/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 360ms/step - acc: 0.8619 - loss: 0.3510 - val_acc: 0.8194 - val_loss: 0.4076\n",
      "Epoch 4/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 371ms/step - acc: 0.9136 - loss: 0.2411 - val_acc: 0.7695 - val_loss: 0.4910\n",
      "Epoch 5/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 348ms/step - acc: 0.8975 - loss: 0.2625 - val_acc: 0.8115 - val_loss: 0.4779\n",
      "Epoch 6/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 361ms/step - acc: 0.9357 - loss: 0.1825 - val_acc: 0.8134 - val_loss: 0.4690\n",
      "Epoch 7/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 363ms/step - acc: 0.9445 - loss: 0.1640 - val_acc: 0.8275 - val_loss: 0.5188\n",
      "Epoch 8/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 377ms/step - acc: 0.9597 - loss: 0.1282 - val_acc: 0.8250 - val_loss: 0.4985\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples+validations_samples]\n",
    "y_val = labels[training_samples: training_samples + validations_samples]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 64, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights= True)\n",
    "model_checkpoint=ModelCheckpoint('./model/imdb/best_model.keras', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=20, batch_size=64, validation_data=(x_val, y_val), callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6f7d6ee-6bd5-4748-8ad5-2ebb5b0bc53c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 588ms/step\n",
      "Text: I hated this movie. It was terrible and ...\n",
      "Prediction: Negative (Acc: 0.1136)\n",
      "Text: This was the worst film I have ever seen...\n",
      "Prediction: Positive (Acc: 0.8603)\n",
      "Text: I loved this movie. It was fantastic and...\n",
      "Prediction: Positive (Acc: 0.9566)\n",
      "Text: This was the best film I have seen in a ...\n",
      "Prediction: Positive (Acc: 0.9512)\n",
      "Text: I had high hopes for this movie, but it ...\n",
      "Prediction: Negative (Acc: 0.1199)\n",
      "Text: This film was a disaster from start to f...\n",
      "Prediction: Negative (Acc: 0.1145)\n",
      "Text: What an amazing film! The plot was deepl...\n",
      "Prediction: Positive (Acc: 0.9367)\n",
      "Text: I was thoroughly impressed by this film....\n",
      "Prediction: Positive (Acc: 0.9570)\n"
     ]
    }
   ],
   "source": [
    "test_texts = [ #아래 문장은 강의 채널에 올려 놓음\n",
    "    \"I hated this movie. It was terrible and the acting was horrible.\",\n",
    "    \"This was the worst film I have ever seen. Not worth the time.\",\n",
    "    \"I loved this movie. It was fantastic and the acting was great.\",\n",
    "    \"This was the best film I have seen in a long time. Totally worth it.\",\n",
    "    \"I had high hopes for this movie, but it was a complete letdown. The plot made no sense and the characters were flat.\",\n",
    "    \"This film was a disaster from start to finish. The dialogue was awkward, and the pacing was painfully slow.\",\n",
    "    \"What an amazing film! The plot was deeply engaging, and the cinematography was stunning from beginning to end.\",\n",
    "    \"I was thoroughly impressed by this film. The direction, the music, and the performances all came together perfectly.\"\n",
    "]\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "test_data = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "\n",
    "best_model = load_model('./model/imdb/best_model.keras', custom_objects = None, compile=True)\n",
    "predictions = best_model.predict(test_data)\n",
    "\n",
    "for i, test_text in enumerate(test_texts):\n",
    "    print(f\"Text: {test_text[:40]}...\")\n",
    "    print(f\"Prediction: {'Positive' if predictions[i] > 0.5 else 'Negative'} (Acc: {predictions[i][0]:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
